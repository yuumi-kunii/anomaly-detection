{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c805b8",
   "metadata": {},
   "source": [
    "# Detec√ß√£o de Anomalias em Transa√ß√µes de Cart√£o de Cr√©dito\n",
    "## Projeto de Aprendizado de M√°quina - Tema 5: Detec√ß√£o de Anomalias (Fraude)\n",
    "\n",
    "### Introdu√ß√£o\n",
    "\n",
    "**Contextualiza√ß√£o do Problema:**\n",
    "A detec√ß√£o de fraudes em transa√ß√µes de cart√£o de cr√©dito √© um problema cr√≠tico no setor financeiro. Com o aumento das transa√ß√µes digitais, os preju√≠zos causados por fraudes podem alcan√ßar bilh√µes de d√≥lares anualmente. O desafio principal est√° no **desbalanceamento extremo** dos dados, onde transa√ß√µes fraudulentas representam uma fra√ß√£o m√≠nima (< 0.2%) do total.\n",
    "\n",
    "**Relev√¢ncia Pr√°tica:**\n",
    "- **Impacto Financeiro:** Fraudes n√£o detectadas geram perdas significativas para institui√ß√µes financeiras e clientes\n",
    "- **Experi√™ncia do Usu√°rio:** Falsos positivos bloqueiam transa√ß√µes leg√≠timas, causando insatisfa√ß√£o\n",
    "- **Requisitos de Tempo Real:** Sistemas devem decidir em milissegundos se uma transa√ß√£o √© fraudulenta\n",
    "\n",
    "**Objetivos do Trabalho:**\n",
    "1. Realizar an√°lise explorat√≥ria e pr√©-processamento dos dados de transa√ß√µes\n",
    "2. Implementar e comparar 3 algoritmos de detec√ß√£o de anomalias:\n",
    "   - **Isolation Forest** (modelo probabil√≠stico)\n",
    "   - **Local Outlier Factor - LOF** (modelo baseado em densidade)\n",
    "   - **Autoencoder** (modelo de Deep Learning)\n",
    "3. Avaliar m√©tricas adequadas para classes desbalanceadas\n",
    "4. Realizar testes de signific√¢ncia estat√≠stica\n",
    "5. Discutir aplicabilidade real da solu√ß√£o\n",
    "\n",
    "**Dataset:** [Credit Card Fraud Detection - Kaggle](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd002b",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√£o das Bibliotecas Necess√°rias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e047bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score,\n",
    "    precision_score, recall_score, f1_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score, accuracy_score\n",
    ")\n",
    "from scipy import stats\n",
    "from scipy.stats import wilcoxon, ttest_rel\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58425a0",
   "metadata": {},
   "source": [
    "## 2. Carregamento e Explora√ß√£o Inicial dos Dados\n",
    "\n",
    "**Descri√ß√£o do Dataset:**\n",
    "O dataset cont√©m transa√ß√µes de cart√µes de cr√©dito realizadas por titulares europeus em setembro de 2013. \n",
    "- **284.807 transa√ß√µes** em 2 dias\n",
    "- **492 fraudes** (0.172% do total)\n",
    "- Features V1-V28 s√£o resultado de transforma√ß√£o PCA (por confidencialidade)\n",
    "- Features 'Time' e 'Amount' n√£o foram transformadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44935db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Link: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(\"INFORMA√á√ïES GERAIS DO DATASET\")\n",
    "print(f\"\\nShape do dataset: {df.shape}\")\n",
    "print(f\"   - Total de transa√ß√µes: {df.shape[0]:,}\")\n",
    "print(f\"   - Total de features: {df.shape[1]}\")\n",
    "print(f\"\\nTipos de dados:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(f\"\\nPrimeiras 5 linhas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ESTAT√çSTICAS DESCRITIVAS\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c0f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"QUALIDADE DOS DADOS\")\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"\\nValores Faltantes:\")\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"   N√£o h√° valores faltantes no dataset!\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])\n",
    "\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nLinhas Duplicadas: {duplicates:,}\")\n",
    "if duplicates > 0:\n",
    "    print(f\"   {duplicates} linhas duplicadas encontradas\")\n",
    "    # Remover duplicados\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"   Duplicados removidos. Novo shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"   N√£o h√° linhas duplicadas!\")\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o da Classe Target (Class):\")\n",
    "print(df['Class'].value_counts())\n",
    "print(f\"\\n   Propor√ß√£o de Fraudes: {df['Class'].mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f01b2b",
   "metadata": {},
   "source": [
    "## 3. An√°lise Explorat√≥ria de Dados (EDA)\n",
    "\n",
    "### 3.1 An√°lise do Desbalanceamento de Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ca443",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "class_counts = df['Class'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[0].bar(['Normal (0)', 'Fraude (1)'], class_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_title('Distribui√ß√£o das Classes', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('N√∫mero de Transa√ß√µes', fontsize=12)\n",
    "axes[0].set_xlabel('Classe', fontsize=12)\n",
    "\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "                 f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "explode = (0, 0.1)\n",
    "axes[1].pie(class_counts.values, explode=explode, labels=['Normal', 'Fraude'], \n",
    "            colors=colors, autopct='%1.3f%%', shadow=True, startangle=90,\n",
    "            textprops={'fontsize': 12})\n",
    "axes[1].set_title('Propor√ß√£o das Classes', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"AN√ÅLISE DO DESBALANCEAMENTO\")\n",
    "fraud_count = df[df['Class'] == 1].shape[0]\n",
    "normal_count = df[df['Class'] == 0].shape[0]\n",
    "print(f\"\\nTransa√ß√µes Normais: {normal_count:,} ({normal_count/len(df)*100:.3f}%)\")\n",
    "print(f\"Transa√ß√µes Fraudulentas: {fraud_count:,} ({fraud_count/len(df)*100:.3f}%)\")\n",
    "print(f\"Raz√£o de Desbalanceamento: 1:{normal_count//fraud_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4ca21",
   "metadata": {},
   "source": [
    "### 3.2 An√°lise das Features Time e Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3321264b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "fraud_df = df[df['Class'] == 1]\n",
    "normal_df = df[df['Class'] == 0]\n",
    "\n",
    "axes[0, 0].hist(normal_df['Time'], bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
    "axes[0, 0].hist(fraud_df['Time'], bins=50, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
    "axes[0, 0].set_title('Distribui√ß√£o de Time por Classe', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (segundos)')\n",
    "axes[0, 0].set_ylabel('Densidade')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].hist(normal_df['Amount'], bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
    "axes[0, 1].hist(fraud_df['Amount'], bins=50, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
    "axes[0, 1].set_title('Distribui√ß√£o de Amount por Classe', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Amount ($)')\n",
    "axes[0, 1].set_ylabel('Densidade')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_xlim(0, 2000)  \n",
    "\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Boxplot de Amount por Classe', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Classe')\n",
    "axes[1, 0].set_ylabel('Amount ($)')\n",
    "axes[1, 0].set_ylim(0, 500)\n",
    "plt.suptitle('')\n",
    "\n",
    "df['Hour'] = (df['Time'] / 3600) % 24\n",
    "fraud_hours = df[df['Class'] == 1]['Hour']\n",
    "normal_hours = df[df['Class'] == 0]['Hour']\n",
    "\n",
    "axes[1, 1].hist(normal_hours, bins=24, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
    "axes[1, 1].hist(fraud_hours, bins=24, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
    "axes[1, 1].set_title('Distribui√ß√£o por Hora do Dia', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Hora do Dia')\n",
    "axes[1, 1].set_ylabel('Densidade')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_amount_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ESTAT√çSTICAS DE TIME E AMOUNT\")\n",
    "print(\"\\nAmount - Transa√ß√µes Normais:\")\n",
    "print(f\"   M√©dia: ${normal_df['Amount'].mean():.2f}\")\n",
    "print(f\"   Mediana: ${normal_df['Amount'].median():.2f}\")\n",
    "print(f\"   Desvio Padr√£o: ${normal_df['Amount'].std():.2f}\")\n",
    "print(f\"   M√°ximo: ${normal_df['Amount'].max():.2f}\")\n",
    "\n",
    "print(\"\\nAmount - Transa√ß√µes Fraudulentas:\")\n",
    "print(f\"   M√©dia: ${fraud_df['Amount'].mean():.2f}\")\n",
    "print(f\"   Mediana: ${fraud_df['Amount'].median():.2f}\")\n",
    "print(f\"   Desvio Padr√£o: ${fraud_df['Amount'].std():.2f}\")\n",
    "print(f\"   M√°ximo: ${fraud_df['Amount'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43ba35",
   "metadata": {},
   "source": [
    "### 3.3 An√°lise das Features PCA (V1-V28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903db5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features = [f'V{i}' for i in range(1, 29)]\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(v_features[:16]):\n",
    "    axes[i].hist(normal_df[feature], bins=50, alpha=0.6, label='Normal', color='#2ecc71', density=True)\n",
    "    axes[i].hist(fraud_df[feature], bins=50, alpha=0.6, label='Fraude', color='#e74c3c', density=True)\n",
    "    axes[i].set_title(f'Distribui√ß√£o de {feature}', fontsize=10)\n",
    "    axes[i].legend(fontsize=8)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('v_features_distribution_1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(v_features[12:28]):\n",
    "    axes[i].hist(normal_df[feature], bins=50, alpha=0.6, label='Normal', color='#2ecc71', density=True)\n",
    "    axes[i].hist(fraud_df[feature], bins=50, alpha=0.6, label='Fraude', color='#e74c3c', density=True)\n",
    "    axes[i].set_title(f'Distribui√ß√£o de {feature}', fontsize=10)\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v_features_distribution_2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18455482",
   "metadata": {},
   "source": [
    "### 3.4 Matriz de Correla√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(24, 20))\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='RdBu_r', \n",
    "            center=0, linewidths=0.5, fmt='.2f',\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Matriz de Correla√ß√£o das Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"CORRELA√á√ÉO DAS FEATURES COM A CLASSE TARGET\")\n",
    "correlations_with_class = df.corr()['Class'].drop('Class').sort_values(key=abs, ascending=False)\n",
    "print(\"\\nTop 10 correla√ß√µes mais fortes (em valor absoluto):\")\n",
    "print(correlations_with_class.head(10))\n",
    "print(\"\\nTop 10 correla√ß√µes mais fracas:\")\n",
    "print(correlations_with_class.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3eadc5",
   "metadata": {},
   "source": [
    "## 4. Pr√©-processamento dos Dados\n",
    "\n",
    "### 4.1 Normaliza√ß√£o das Features\n",
    "\n",
    "As features V1-V28 j√° est√£o normalizadas (resultado de PCA), mas **Time** e **Amount** precisam ser normalizadas.\n",
    "Utilizaremos o **RobustScaler** para Amount (menos sens√≠vel a outliers) e **StandardScaler** para Time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562285ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "\n",
    "if 'Hour' in df_processed.columns:\n",
    "    df_processed = df_processed.drop('Hour', axis=1)\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "df_processed['Amount_scaled'] = robust_scaler.fit_transform(df_processed['Amount'].values.reshape(-1, 1))\n",
    "\n",
    "df_processed['Time_scaled'] = standard_scaler.fit_transform(df_processed['Time'].values.reshape(-1, 1))\n",
    "\n",
    "df_processed = df_processed.drop(['Time', 'Amount'], axis=1)\n",
    "\n",
    "print(\"Normaliza√ß√£o conclu√≠da!\")\n",
    "print(f\"\\nShape do dataset processado: {df_processed.shape}\")\n",
    "print(f\"\\nColunas ap√≥s pr√©-processamento:\")\n",
    "print(df_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5523d60d",
   "metadata": {},
   "source": [
    "### 4.2 Separa√ß√£o dos Dados em Treino, Valida√ß√£o e Teste\n",
    "\n",
    "Utilizaremos a seguinte divis√£o:\n",
    "- **Treino:** 70% dos dados\n",
    "- **Valida√ß√£o:** 15% dos dados  \n",
    "- **Teste:** 15% dos dados\n",
    "\n",
    "**Importante:** Para detec√ß√£o de anomalias em abordagem semi-supervisionada, treinaremos os modelos apenas com transa√ß√µes **normais**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f14669",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop('Class', axis=1)\n",
    "y = df_processed['Class']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"DIVIS√ÉO DOS DADOS\")\n",
    "\n",
    "print(f\"\\nConjunto de TREINO:\")\n",
    "print(f\"   Total: {len(X_train):,} amostras ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Normal: {(y_train == 0).sum():,}\")\n",
    "print(f\"   Fraude: {(y_train == 1).sum():,}\")\n",
    "\n",
    "print(f\"\\nConjunto de VALIDA√á√ÉO:\")\n",
    "print(f\"   Total: {len(X_val):,} amostras ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Normal: {(y_val == 0).sum():,}\")\n",
    "print(f\"   Fraude: {(y_val == 1).sum():,}\")\n",
    "\n",
    "print(f\"\\nConjunto de TESTE:\")\n",
    "print(f\"   Total: {len(X_test):,} amostras ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Normal: {(y_test == 0).sum():,}\")\n",
    "print(f\"   Fraude: {(y_test == 1).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_normal = X_train[y_train == 0]\n",
    "y_train_normal = y_train[y_train == 0]\n",
    "\n",
    "\n",
    "print(\"CONJUNTO DE TREINO PARA ABORDAGEM SEMI-SUPERVISIONADA\")\n",
    "\n",
    "print(f\"\\nüìä Treino apenas com transa√ß√µes NORMAIS:\")\n",
    "print(f\"   Total: {len(X_train_normal):,} amostras\")\n",
    "print(f\"   (Usaremos este conjunto para treinar os modelos de anomalia)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854bc965",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering e Sele√ß√£o de Features\n",
    "\n",
    "### 5.1 An√°lise de Import√¢ncia das Features\n",
    "\n",
    "As features V1-V28 j√° s√£o resultado de PCA, aplicado para manter a confidencialidade dos dados originais. Vamos analisar quais features apresentam maior diferen√ßa entre classes para auxiliar na interpreta√ß√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = []\n",
    "features = X.columns.tolist()\n",
    "\n",
    "for feature in features:\n",
    "    fraud_mean = df_processed[df_processed['Class'] == 1][feature].mean()\n",
    "    normal_mean = df_processed[df_processed['Class'] == 0][feature].mean()\n",
    "    fraud_std = df_processed[df_processed['Class'] == 1][feature].std()\n",
    "    normal_std = df_processed[df_processed['Class'] == 0][feature].std()\n",
    "    \n",
    "\n",
    "    pooled_std = np.sqrt((fraud_std**2 + normal_std**2) / 2)\n",
    "    if pooled_std > 0:\n",
    "        effect_size = abs(fraud_mean - normal_mean) / pooled_std\n",
    "    else:\n",
    "        effect_size = 0\n",
    "    \n",
    "    feature_importance.append({\n",
    "        'Feature': feature,\n",
    "        'Fraud_Mean': fraud_mean,\n",
    "        'Normal_Mean': normal_mean,\n",
    "        'Effect_Size': effect_size\n",
    "    })\n",
    "\n",
    "importance_df = pd.DataFrame(feature_importance)\n",
    "importance_df = importance_df.sort_values('Effect_Size', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "top_features = importance_df.head(15)\n",
    "bars = plt.barh(top_features['Feature'], top_features['Effect_Size'], color='steelblue')\n",
    "plt.xlabel('Effect Size (Cohen\\'s d)', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 15 Features por Diferen√ßa entre Classes (Effect Size)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Features com maior diferen√ßa entre classes:\")\n",
    "print(importance_df[['Feature', 'Effect_Size']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98a5cd",
   "metadata": {},
   "source": [
    "### 5.2 Sele√ß√£o de Features\n",
    "\n",
    "Para este projeto, utilizaremos **todas as features** dispon√≠veis, pois:\n",
    "1. As features V1-V28 j√° s√£o componentes principais otimizados\n",
    "2. Mesmo features com baixa correla√ß√£o individual podem contribuir em combina√ß√£o\n",
    "3. Os algoritmos de detec√ß√£o de anomalias s√£o capazes de lidar com m√∫ltiplas dimens√µes\n",
    "\n",
    "Caso seja necess√°rio reduzir dimensionalidade, as features com maior effect size seriam priorizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd037f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEATURES SELECIONADAS PARA MODELAGEM\")\n",
    "\n",
    "print(f\"\\nTotal de features: {X.shape[1]}\")\n",
    "print(f\"\\nLista de features:\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5391bdb8",
   "metadata": {},
   "source": [
    "## 6. Modelagem\n",
    "\n",
    "### 6.1 Fun√ß√µes Auxiliares para Avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed913a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_scores=None, model_name=\"Model\"):\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "    \n",
    "    if y_scores is not None:\n",
    "        metrics['AUC-ROC'] = roc_auc_score(y_true, y_scores)\n",
    "        metrics['AUC-PR'] = average_precision_score(y_true, y_scores)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\", ax=None):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Normal', 'Fraude'],\n",
    "                yticklabels=['Normal', 'Fraude'])\n",
    "    ax.set_xlabel('Predito')\n",
    "    ax.set_ylabel('Real')\n",
    "    ax.set_title(f'Matriz de Confus√£o - {model_name}')\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "def print_metrics(metrics):\n",
    "    print(f\"M√âTRICAS - {metrics['Model']}\")\n",
    "    print(f\"  Acur√°cia:  {metrics['Accuracy']:.4f}\")\n",
    "    print(f\"  Precis√£o:  {metrics['Precision']:.4f}\")\n",
    "    print(f\"  Recall:    {metrics['Recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {metrics['F1-Score']:.4f}\")\n",
    "    if 'AUC-ROC' in metrics:\n",
    "        print(f\"  AUC-ROC:   {metrics['AUC-ROC']:.4f}\")\n",
    "    if 'AUC-PR' in metrics:\n",
    "        print(f\"  AUC-PR:    {metrics['AUC-PR']:.4f}\")\n",
    "\n",
    "all_results = {}\n",
    "all_predictions = {}\n",
    "all_scores = {}\n",
    "all_times = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534911ee",
   "metadata": {},
   "source": [
    "### 6.2 Modelo 1: Isolation Forest (Modelo Probabil√≠stico)\n",
    "\n",
    "**Por que Isolation Forest?**\n",
    "- Baseado no princ√≠pio de que anomalias s√£o \"isoladas\" mais facilmente\n",
    "- N√£o assume uma distribui√ß√£o espec√≠fica dos dados\n",
    "- Eficiente computacionalmente (O(n log n))\n",
    "- Funciona bem com dados de alta dimensionalidade\n",
    "- Ideal para detec√ß√£o de outliers em datasets desbalanceados\n",
    "\n",
    "**Funcionamento:**\n",
    "1. Constr√≥i √°rvores de decis√£o aleat√≥rias\n",
    "2. Anomalias requerem menos \"cortes\" para serem isoladas\n",
    "3. O score de anomalia √© baseado no caminho m√©dio at√© o isolamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_rate = y_val.mean()\n",
    "print(f\"Taxa de contamina√ß√£o esperada: {contamination_rate:.4f} ({contamination_rate*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TREINAMENTO - ISOLATION FOREST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if_params = {\n",
    "    'n_estimators': 100,\n",
    "    'contamination': contamination_rate,\n",
    "    'max_samples': 'auto',\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "iso_forest = IsolationForest(**if_params)\n",
    "iso_forest.fit(X_train_normal) \n",
    "train_time_if = time.time() - start_time\n",
    "\n",
    "print(f\"Modelo treinado em {train_time_if:.2f} segundos\")\n",
    "\n",
    "start_time = time.time()\n",
    "if_val_pred_raw = iso_forest.predict(X_val)\n",
    "if_val_pred = np.where(if_val_pred_raw == -1, 1, 0)  \n",
    "if_val_scores = -iso_forest.score_samples(X_val) \n",
    "inference_time_if_val = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de infer√™ncia (valida√ß√£o): {inference_time_if_val:.2f} segundos\")\n",
    "\n",
    "if_val_metrics = evaluate_model(y_val, if_val_pred, if_val_scores, \"Isolation Forest\")\n",
    "print_metrics(if_val_metrics)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(y_val, if_val_pred, \"Isolation Forest (Valida√ß√£o)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_if_val.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6ef0c",
   "metadata": {},
   "source": [
    "#### 6.2.1 Tunagem de Hiperpar√¢metros - Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b486e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TUNAGEM DE HIPERPAR√ÇMETROS - ISOLATION FOREST\")\n",
    "\n",
    "n_estimators_list = [50, 100, 200]\n",
    "max_samples_list = [256, 512, 'auto']\n",
    "contamination_list = [0.001, 0.002, 0.005, contamination_rate]\n",
    "\n",
    "best_if_score = 0\n",
    "best_if_params = {}\n",
    "if_tuning_results = []\n",
    "\n",
    "for n_est in n_estimators_list:\n",
    "    for max_samp in max_samples_list:\n",
    "        for contam in contamination_list:\n",
    "            try:\n",
    "                model = IsolationForest(\n",
    "                    n_estimators=n_est,\n",
    "                    max_samples=max_samp,\n",
    "                    contamination=contam,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                model.fit(X_train_normal)\n",
    "                \n",
    "                pred_raw = model.predict(X_val)\n",
    "                pred = np.where(pred_raw == -1, 1, 0)\n",
    "                scores = -model.score_samples(X_val)\n",
    "                \n",
    "                f1 = f1_score(y_val, pred, zero_division=0)\n",
    "                auc = roc_auc_score(y_val, scores)\n",
    "                \n",
    "                if_tuning_results.append({\n",
    "                    'n_estimators': n_est,\n",
    "                    'max_samples': max_samp,\n",
    "                    'contamination': contam,\n",
    "                    'F1-Score': f1,\n",
    "                    'AUC-ROC': auc\n",
    "                })\n",
    "                \n",
    "                if f1 > best_if_score:\n",
    "                    best_if_score = f1\n",
    "                    best_if_params = {\n",
    "                        'n_estimators': n_est,\n",
    "                        'max_samples': max_samp,\n",
    "                        'contamination': contam\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                print(f\"Erro com params {n_est}, {max_samp}, {contam}: {e}\")\n",
    "\n",
    "if_tuning_df = pd.DataFrame(if_tuning_results).sort_values('F1-Score', ascending=False)\n",
    "print(\"\\nTop 10 combina√ß√µes de hiperpar√¢metros:\")\n",
    "print(if_tuning_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nMelhores hiperpar√¢metros encontrados:\")\n",
    "print(f\"   n_estimators: {best_if_params['n_estimators']}\")\n",
    "print(f\"   max_samples: {best_if_params['max_samples']}\")\n",
    "print(f\"   contamination: {best_if_params['contamination']}\")\n",
    "print(f\"   F1-Score: {best_if_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MODELO FINAL - ISOLATION FOREST\")\n",
    "\n",
    "iso_forest_best = IsolationForest(\n",
    "    n_estimators=best_if_params['n_estimators'],\n",
    "    max_samples=best_if_params['max_samples'],\n",
    "    contamination=best_if_params['contamination'],\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "iso_forest_best.fit(X_train_normal)\n",
    "train_time_if_best = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "if_test_pred_raw = iso_forest_best.predict(X_test)\n",
    "if_test_pred = np.where(if_test_pred_raw == -1, 1, 0)\n",
    "if_test_scores = -iso_forest_best.score_samples(X_test)\n",
    "inference_time_if = time.time() - start_time\n",
    "\n",
    "if_test_metrics = evaluate_model(y_test, if_test_pred, if_test_scores, \"Isolation Forest (Best)\")\n",
    "print_metrics(if_test_metrics)\n",
    "print(f\"\\nTempo de treino: {train_time_if_best:.2f}s\")\n",
    "print(f\"Tempo de infer√™ncia (teste): {inference_time_if:.4f}s\")\n",
    "print(f\"Tempo m√©dio por amostra: {inference_time_if/len(X_test)*1000:.4f}ms\")\n",
    "\n",
    "all_results['Isolation Forest'] = if_test_metrics\n",
    "all_predictions['Isolation Forest'] = if_test_pred\n",
    "all_scores['Isolation Forest'] = if_test_scores\n",
    "all_times['Isolation Forest'] = {\n",
    "    'train': train_time_if_best,\n",
    "    'inference': inference_time_if,\n",
    "    'per_sample': inference_time_if/len(X_test)*1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbc5743",
   "metadata": {},
   "source": [
    "### 6.3 Modelo 2: Local Outlier Factor - LOF (Modelo Baseado em Densidade)\n",
    "\n",
    "**Por que LOF?**\n",
    "- Algoritmo baseado em densidade local\n",
    "- Detecta anomalias considerando a densidade dos vizinhos\n",
    "- Captura anomalias locais que podem ser diferentes em diferentes regi√µes do espa√ßo\n",
    "- N√£o assume forma espec√≠fica dos clusters\n",
    "\n",
    "**Funcionamento:**\n",
    "1. Calcula a densidade local de cada ponto baseado em k vizinhos\n",
    "2. Compara a densidade de um ponto com a de seus vizinhos\n",
    "3. Pontos com densidade significativamente menor s√£o anomalias\n",
    "4. LOF > 1 indica potencial anomalia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4645569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lof_params = {\n",
    "    'n_neighbors': 20,\n",
    "    'contamination': contamination_rate,\n",
    "    'novelty': True,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "lof = LocalOutlierFactor(**lof_params)\n",
    "lof.fit(X_train_normal)\n",
    "train_time_lof = time.time() - start_time\n",
    "\n",
    "print(f\"Modelo treinado em {train_time_lof:.2f} segundos\")\n",
    "\n",
    "start_time = time.time()\n",
    "lof_val_pred_raw = lof.predict(X_val)\n",
    "lof_val_pred = np.where(lof_val_pred_raw == -1, 1, 0)\n",
    "lof_val_scores = -lof.score_samples(X_val)\n",
    "inference_time_lof_val = time.time() - start_time\n",
    "\n",
    "print(f\"Tempo de infer√™ncia (valida√ß√£o): {inference_time_lof_val:.2f} segundos\")\n",
    "\n",
    "lof_val_metrics = evaluate_model(y_val, lof_val_pred, lof_val_scores, \"LOF\")\n",
    "print_metrics(lof_val_metrics)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(y_val, lof_val_pred, \"LOF (Valida√ß√£o)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_lof_val.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf2a5e7",
   "metadata": {},
   "source": [
    "#### 6.3.1 Tunagem de Hiperpar√¢metros - LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ca15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_neighbors_list = [5, 10, 20, 30, 50]\n",
    "contamination_list_lof = [0.001, 0.002, 0.005, contamination_rate]\n",
    "\n",
    "best_lof_score = 0\n",
    "best_lof_params = {}\n",
    "lof_tuning_results = []\n",
    "\n",
    "for n_neigh in n_neighbors_list:\n",
    "    for contam in contamination_list_lof:\n",
    "        try:\n",
    "            model = LocalOutlierFactor(\n",
    "                n_neighbors=n_neigh,\n",
    "                contamination=contam,\n",
    "                novelty=True,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            model.fit(X_train_normal)\n",
    "            \n",
    "            pred_raw = model.predict(X_val)\n",
    "            pred = np.where(pred_raw == -1, 1, 0)\n",
    "            scores = -model.score_samples(X_val)\n",
    "            \n",
    "            f1 = f1_score(y_val, pred, zero_division=0)\n",
    "            auc = roc_auc_score(y_val, scores)\n",
    "            \n",
    "            lof_tuning_results.append({\n",
    "                'n_neighbors': n_neigh,\n",
    "                'contamination': contam,\n",
    "                'F1-Score': f1,\n",
    "                'AUC-ROC': auc\n",
    "            })\n",
    "            \n",
    "            if f1 > best_lof_score:\n",
    "                best_lof_score = f1\n",
    "                best_lof_params = {\n",
    "                    'n_neighbors': n_neigh,\n",
    "                    'contamination': contam\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Erro com params {n_neigh}, {contam}: {e}\")\n",
    "\n",
    "lof_tuning_df = pd.DataFrame(lof_tuning_results).sort_values('F1-Score', ascending=False)\n",
    "print(\"\\nTop 10 combina√ß√µes de hiperpar√¢metros:\")\n",
    "print(lof_tuning_df.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nMelhores hiperpar√¢metros encontrados:\")\n",
    "print(f\"   n_neighbors: {best_lof_params['n_neighbors']}\")\n",
    "print(f\"   contamination: {best_lof_params['contamination']}\")\n",
    "print(f\"   F1-Score: {best_lof_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dfe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"MODELO FINAL - LOF\")\n",
    "\n",
    "lof_best = LocalOutlierFactor(\n",
    "    n_neighbors=best_lof_params['n_neighbors'],\n",
    "    contamination=best_lof_params['contamination'],\n",
    "    novelty=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "lof_best.fit(X_train_normal)\n",
    "train_time_lof_best = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "lof_test_pred_raw = lof_best.predict(X_test)\n",
    "lof_test_pred = np.where(lof_test_pred_raw == -1, 1, 0)\n",
    "lof_test_scores = -lof_best.score_samples(X_test)\n",
    "inference_time_lof = time.time() - start_time\n",
    "\n",
    "lof_test_metrics = evaluate_model(y_test, lof_test_pred, lof_test_scores, \"LOF (Best)\")\n",
    "print_metrics(lof_test_metrics)\n",
    "print(f\"\\nTempo de treino: {train_time_lof_best:.2f}s\")\n",
    "print(f\"Tempo de infer√™ncia (teste): {inference_time_lof:.4f}s\")\n",
    "print(f\"Tempo m√©dio por amostra: {inference_time_lof/len(X_test)*1000:.4f}ms\")\n",
    "\n",
    "all_results['LOF'] = lof_test_metrics\n",
    "all_predictions['LOF'] = lof_test_pred\n",
    "all_scores['LOF'] = lof_test_scores\n",
    "all_times['LOF'] = {\n",
    "    'train': train_time_lof_best,\n",
    "    'inference': inference_time_lof,\n",
    "    'per_sample': inference_time_lof/len(X_test)*1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b0b54",
   "metadata": {},
   "source": [
    "### 6.4 Modelo 3: Autoencoder (Deep Learning)\n",
    "\n",
    "**Por que Autoencoder?**\n",
    "- Rede neural que aprende a reconstruir os dados de entrada\n",
    "- Treinado apenas com dados normais, aprende a \"representa√ß√£o\" de normalidade\n",
    "- Anomalias t√™m alto erro de reconstru√ß√£o (n√£o se encaixam no padr√£o aprendido)\n",
    "- Capaz de capturar rela√ß√µes n√£o-lineares complexas entre features\n",
    "\n",
    "**Arquitetura:**\n",
    "- **Encoder:** Comprime os dados para uma representa√ß√£o latente\n",
    "- **Decoder:** Reconstr√≥i os dados a partir da representa√ß√£o latente\n",
    "- **Detec√ß√£o:** Baseada no erro de reconstru√ß√£o (MSE)\n",
    "\n",
    "**Treinamento:**\n",
    "- Treinar apenas com transa√ß√µes normais\n",
    "- Validar com dados que incluem fraudes\n",
    "- Threshold definido baseado no percentil do erro de reconstru√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ff6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_dim, encoding_dim=14, hidden_layers=[28, 21]):\n",
    "\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    x = input_layer\n",
    "    for units in hidden_layers:\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    latent = Dense(encoding_dim, activation='relu', name='latent')(x)\n",
    "    \n",
    "    x = latent\n",
    "    for units in reversed(hidden_layers):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    \n",
    "    output_layer = Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer, name='autoencoder')\n",
    "    encoder = Model(inputs=input_layer, outputs=latent, name='encoder')\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "\n",
    "def get_reconstruction_error(model, data):\n",
    "    reconstructed = model.predict(data, verbose=0)\n",
    "    mse = np.mean(np.power(data - reconstructed, 2), axis=1)\n",
    "    return mse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim = X_train_normal.shape[1]\n",
    "encoding_dim = 14\n",
    "hidden_layers = [28, 21]\n",
    "\n",
    "autoencoder, encoder = build_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "\n",
    "autoencoder.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "print(\"\\nArquitetura do Autoencoder:\")\n",
    "autoencoder.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nIniciando treinamento...\")\n",
    "start_time = time.time()\n",
    "history = autoencoder.fit(\n",
    "    X_train_normal.values,\n",
    "    X_train_normal.values,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "train_time_ae = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTreinamento conclu√≠do em {train_time_ae:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fec6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history.history['loss'], label='Treino', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Valida√ß√£o', linewidth=2)\n",
    "axes[0].set_title('Loss do Autoencoder', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('√âpoca')\n",
    "axes[0].set_ylabel('MSE Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "if 'lr' in history.history:\n",
    "    axes[1].plot(history.history['lr'], linewidth=2, color='green')\n",
    "    axes[1].set_title('Learning Rate', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('√âpoca')\n",
    "    axes[1].set_ylabel('LR')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    train_errors = get_reconstruction_error(autoencoder, X_train_normal.values)\n",
    "    axes[1].hist(train_errors, bins=50, alpha=0.7, color='steelblue')\n",
    "    axes[1].set_title('Distribui√ß√£o do Erro de Reconstru√ß√£o (Treino Normal)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('MSE')\n",
    "    axes[1].set_ylabel('Frequ√™ncia')\n",
    "    axes[1].axvline(np.percentile(train_errors, 95), color='red', linestyle='--', label='Percentil 95')\n",
    "    axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoencoder_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a66dd9",
   "metadata": {},
   "source": [
    "#### 6.4.1 Defini√ß√£o do Threshold para Detec√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4d68d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_normal_errors = get_reconstruction_error(autoencoder, X_train_normal.values)\n",
    "val_errors = get_reconstruction_error(autoencoder, X_val.values)\n",
    "\n",
    "val_normal_errors = val_errors[y_val == 0]\n",
    "val_fraud_errors = val_errors[y_val == 1]\n",
    "\n",
    "print(f\"\\nEstat√≠sticas do Erro de Reconstru√ß√£o:\")\n",
    "print(f\"\\n   Treino (Normal):\")\n",
    "print(f\"   - M√©dia: {np.mean(train_normal_errors):.6f}\")\n",
    "print(f\"   - Mediana: {np.median(train_normal_errors):.6f}\")\n",
    "print(f\"   - Std: {np.std(train_normal_errors):.6f}\")\n",
    "print(f\"   - Percentil 95: {np.percentile(train_normal_errors, 95):.6f}\")\n",
    "print(f\"   - Percentil 99: {np.percentile(train_normal_errors, 99):.6f}\")\n",
    "\n",
    "print(f\"\\n   Valida√ß√£o (Normal):\")\n",
    "print(f\"   - M√©dia: {np.mean(val_normal_errors):.6f}\")\n",
    "print(f\"   - Mediana: {np.median(val_normal_errors):.6f}\")\n",
    "\n",
    "print(f\"\\n   Valida√ß√£o (Fraude):\")\n",
    "print(f\"   - M√©dia: {np.mean(val_fraud_errors):.6f}\")\n",
    "print(f\"   - Mediana: {np.median(val_fraud_errors):.6f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "axes[0].hist(val_normal_errors, bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
    "axes[0].hist(val_fraud_errors, bins=50, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
    "axes[0].set_title('Distribui√ß√£o do Erro de Reconstru√ß√£o por Classe', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('MSE (Erro de Reconstru√ß√£o)')\n",
    "axes[0].set_ylabel('Densidade')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, np.percentile(val_errors, 99))\n",
    "\n",
    "val_errors_df = pd.DataFrame({\n",
    "    'Error': val_errors,\n",
    "    'Class': y_val.values\n",
    "})\n",
    "val_errors_df['Class'] = val_errors_df['Class'].map({0: 'Normal', 1: 'Fraude'})\n",
    "val_errors_df.boxplot(column='Error', by='Class', ax=axes[1])\n",
    "axes[1].set_title('Boxplot do Erro de Reconstru√ß√£o por Classe', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Classe')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_ylim(0, np.percentile(val_errors, 99))\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('autoencoder_error_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 99.5]\n",
    "threshold_results = []\n",
    "\n",
    "for perc in percentiles:\n",
    "    threshold = np.percentile(train_normal_errors, perc)\n",
    "    \n",
    "    val_pred = (val_errors > threshold).astype(int)\n",
    "    \n",
    "    prec = precision_score(y_val, val_pred, zero_division=0)\n",
    "    rec = recall_score(y_val, val_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, val_pred, zero_division=0)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Percentile': perc,\n",
    "        'Threshold': threshold,\n",
    "        'Precision': prec,\n",
    "        'Recall': rec,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "threshold_df = pd.DataFrame(threshold_results)\n",
    "print(\"\\nResultados por Threshold (Percentil):\")\n",
    "print(threshold_df.to_string(index=False))\n",
    "\n",
    "best_threshold_idx = threshold_df['F1-Score'].idxmax()\n",
    "best_threshold = threshold_df.loc[best_threshold_idx, 'Threshold']\n",
    "best_percentile = threshold_df.loc[best_threshold_idx, 'Percentile']\n",
    "\n",
    "print(f\"\\nMelhor threshold encontrado:\")\n",
    "print(f\"   Percentil: {best_percentile}\")\n",
    "print(f\"   Threshold: {best_threshold:.6f}\")\n",
    "print(f\"   F1-Score: {threshold_df.loc[best_threshold_idx, 'F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d778f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "test_errors = get_reconstruction_error(autoencoder, X_test.values)\n",
    "inference_time_ae = time.time() - start_time\n",
    "\n",
    "ae_test_pred = (test_errors > best_threshold).astype(int)\n",
    "ae_test_scores = test_errors \n",
    "\n",
    "ae_test_metrics = evaluate_model(y_test, ae_test_pred, ae_test_scores, \"Autoencoder (Best)\")\n",
    "print_metrics(ae_test_metrics)\n",
    "print(f\"\\nTempo de treino: {train_time_ae:.2f}s\")\n",
    "print(f\"Tempo de infer√™ncia (teste): {inference_time_ae:.4f}s\")\n",
    "print(f\"Tempo m√©dio por amostra: {inference_time_ae/len(X_test)*1000:.4f}ms\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_confusion_matrix(y_test, ae_test_pred, \"Autoencoder (Teste)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_ae_test.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "all_results['Autoencoder'] = ae_test_metrics\n",
    "all_predictions['Autoencoder'] = ae_test_pred\n",
    "all_scores['Autoencoder'] = ae_test_scores\n",
    "all_times['Autoencoder'] = {\n",
    "    'train': train_time_ae,\n",
    "    'inference': inference_time_ae,\n",
    "    'per_sample': inference_time_ae/len(X_test)*1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f497d",
   "metadata": {},
   "source": [
    "## 7. Compara√ß√£o de Resultados\n",
    "\n",
    "### 7.1 Tabela Comparativa de M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([\n",
    "    all_results['Isolation Forest'],\n",
    "    all_results['LOF'],\n",
    "    all_results['Autoencoder']\n",
    "])\n",
    "\n",
    "results_df['Train Time (s)'] = [\n",
    "    all_times['Isolation Forest']['train'],\n",
    "    all_times['LOF']['train'],\n",
    "    all_times['Autoencoder']['train']\n",
    "]\n",
    "results_df['Inference Time (s)'] = [\n",
    "    all_times['Isolation Forest']['inference'],\n",
    "    all_times['LOF']['inference'],\n",
    "    all_times['Autoencoder']['inference']\n",
    "]\n",
    "results_df['Time/Sample (ms)'] = [\n",
    "    all_times['Isolation Forest']['per_sample'],\n",
    "    all_times['LOF']['per_sample'],\n",
    "    all_times['Autoencoder']['per_sample']\n",
    "]\n",
    "\n",
    "results_df['Model'] = ['Isolation Forest', 'LOF', 'Autoencoder']\n",
    "cols = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'AUC-PR', \n",
    "        'Train Time (s)', 'Inference Time (s)', 'Time/Sample (ms)']\n",
    "results_df = results_df[cols]\n",
    "\n",
    "print(\"\\nM√©tricas de Desempenho:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "results_df.to_csv('model_comparison_results.csv', index=False)\n",
    "print(\"\\nResultados salvos em 'model_comparison_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102090af",
   "metadata": {},
   "source": [
    "### 7.2 Visualiza√ß√£o Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "\n",
    "metrics_to_plot = ['Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "models = ['Isolation Forest', 'LOF', 'Autoencoder']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    values = [results_df[results_df['Model'] == model][m].values[0] for m in metrics_to_plot]\n",
    "    bars = axes[0, 0].bar(x + i*width, values, width, label=model, color=colors[i])\n",
    "    \n",
    "axes[0, 0].set_xlabel('M√©trica')\n",
    "axes[0, 0].set_ylabel('Valor')\n",
    "axes[0, 0].set_title('Compara√ß√£o de M√©tricas por Modelo', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x + width)\n",
    "axes[0, 0].set_xticklabels(metrics_to_plot)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    scores = all_scores[model]\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    auc = roc_auc_score(y_test, scores)\n",
    "    axes[0, 1].plot(fpr, tpr, label=f'{model} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[0, 1].set_xlabel('Taxa de Falsos Positivos (FPR)')\n",
    "axes[0, 1].set_ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
    "axes[0, 1].set_title('Curvas ROC', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(loc='lower right')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for model, color in zip(models, colors):\n",
    "    scores = all_scores[model]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, scores)\n",
    "    ap = average_precision_score(y_test, scores)\n",
    "    axes[1, 0].plot(recall, precision, label=f'{model} (AP={ap:.3f})', color=color, linewidth=2)\n",
    "\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('Curvas Precision-Recall', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(loc='upper right')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "times = [all_times[m]['per_sample'] for m in models]\n",
    "bars = axes[1, 1].bar(models, times, color=colors)\n",
    "axes[1, 1].set_xlabel('Modelo')\n",
    "axes[1, 1].set_ylabel('Tempo por Amostra (ms)')\n",
    "axes[1, 1].set_title('Tempo de Infer√™ncia por Amostra', fontsize=14, fontweight='bold')\n",
    "for bar, t in zip(bars, times):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height(), \n",
    "                    f'{t:.4f}ms', ha='center', va='bottom', fontsize=10)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f01e83",
   "metadata": {},
   "source": [
    "### 7.3 Matrizes de Confus√£o Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for ax, model in zip(axes, models):\n",
    "    plot_confusion_matrix(y_test, all_predictions[model], model, ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrices_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    cm = confusion_matrix(y_test, all_predictions[model])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"   True Negatives (TN): {tn:,} - Normais classificados corretamente\")\n",
    "    print(f\"   False Positives (FP): {fp:,} - Normais classificados como fraude\")\n",
    "    print(f\"   False Negatives (FN): {fn:,} - Fraudes n√£o detectadas\")\n",
    "    print(f\"   True Positives (TP): {tp:,} - Fraudes detectadas\")\n",
    "    \n",
    "    fraud_detection_rate = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0\n",
    "    false_alarm_rate = fp / (fp + tn) * 100 if (fp + tn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n   Taxa de Detec√ß√£o de Fraudes: {fraud_detection_rate:.2f}%\")\n",
    "    print(f\"   Taxa de Falso Alarme: {false_alarm_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5ffc4",
   "metadata": {},
   "source": [
    "## 8. Testes de Signific√¢ncia Estat√≠stica\n",
    "\n",
    "Para validar se as diferen√ßas de desempenho entre os modelos s√£o estatisticamente significativas, utilizaremos o **Teste de McNemar**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabfa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcnemar_test(y_true, pred1, pred2, model1_name, model2_name):\n",
    "\n",
    "    correct1 = (pred1 == y_true)\n",
    "    correct2 = (pred2 == y_true)\n",
    "    \n",
    "    b = np.sum(correct1 & ~correct2)  \n",
    "    c = np.sum(~correct1 & correct2)  \n",
    "    \n",
    "    if b + c == 0:\n",
    "        return None, None, \"N√£o foi poss√≠vel calcular (b+c=0)\"\n",
    "    \n",
    "    chi2 = (abs(b - c) - 1)**2 / (b + c)\n",
    "    p_value = 1 - stats.chi2.cdf(chi2, df=1)\n",
    "    \n",
    "    return chi2, p_value, f\"b={b}, c={c}\"\n",
    "\n",
    "model_pairs = [\n",
    "    ('Isolation Forest', 'LOF'),\n",
    "    ('Isolation Forest', 'Autoencoder'),\n",
    "    ('LOF', 'Autoencoder')\n",
    "]\n",
    "\n",
    "significance_results = []\n",
    "\n",
    "for model1, model2 in model_pairs:\n",
    "    chi2, p_value, details = mcnemar_test(\n",
    "        y_test.values,\n",
    "        all_predictions[model1],\n",
    "        all_predictions[model2],\n",
    "        model1, model2\n",
    "    )\n",
    "    \n",
    "    if chi2 is not None:\n",
    "        significant = \"Sim\" if p_value < 0.05 else \"N√£o\"\n",
    "        significance_results.append({\n",
    "            'Modelo 1': model1,\n",
    "            'Modelo 2': model2,\n",
    "            'Chi¬≤': chi2,\n",
    "            'p-value': p_value,\n",
    "            'Significativo (Œ±=0.05)': significant,\n",
    "            'Detalhes': details\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n{model1} vs {model2}:\")\n",
    "        print(f\"   Chi¬≤ = {chi2:.4f}\")\n",
    "        print(f\"   p-value = {p_value:.6f}\")\n",
    "        print(f\"   Diferen√ßa significativa: {significant}\")\n",
    "    else:\n",
    "        print(f\"\\n{model1} vs {model2}: {details}\")\n",
    "\n",
    "significance_df = pd.DataFrame(significance_results)\n",
    "print(\"RESUMO DOS TESTES DE SIGNIFIC√ÇNCIA\")\n",
    "print(significance_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb8c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc(y_true, scores, n_bootstraps=1000, confidence=0.95):\n",
    "    aucs = []\n",
    "    rng = np.random.RandomState(42)\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        indices = rng.randint(0, len(y_true), len(y_true))\n",
    "        if len(np.unique(y_true[indices])) < 2:\n",
    "            continue\n",
    "        auc = roc_auc_score(y_true[indices], scores[indices])\n",
    "        aucs.append(auc)\n",
    "    \n",
    "    alpha = (1 - confidence) / 2\n",
    "    lower = np.percentile(aucs, alpha * 100)\n",
    "    upper = np.percentile(aucs, (1 - alpha) * 100)\n",
    "    mean_auc = np.mean(aucs)\n",
    "    \n",
    "    return mean_auc, lower, upper\n",
    "\n",
    "bootstrap_results = []\n",
    "for model in models:\n",
    "    mean_auc, lower, upper = bootstrap_auc(y_test.values, all_scores[model])\n",
    "    bootstrap_results.append({\n",
    "        'Modelo': model,\n",
    "        'AUC M√©dio': mean_auc,\n",
    "        'IC 95% Inferior': lower,\n",
    "        'IC 95% Superior': upper\n",
    "    })\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"   AUC-ROC: {mean_auc:.4f}\")\n",
    "    print(f\"   IC 95%: [{lower:.4f}, {upper:.4f}]\")\n",
    "\n",
    "bootstrap_df = pd.DataFrame(bootstrap_results)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    row = bootstrap_df[bootstrap_df['Modelo'] == model].iloc[0]\n",
    "    plt.errorbar(i, row['AUC M√©dio'], \n",
    "                 yerr=[[row['AUC M√©dio'] - row['IC 95% Inferior']], \n",
    "                       [row['IC 95% Superior'] - row['AUC M√©dio']]],\n",
    "                 fmt='o', markersize=10, capsize=5, capthick=2, color=colors[i],\n",
    "                 label=model)\n",
    "\n",
    "plt.xticks(range(len(models)), models)\n",
    "plt.ylabel('AUC-ROC')\n",
    "plt.title('Intervalos de Confian√ßa 95% para AUC-ROC', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('auc_confidence_intervals.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250bc66",
   "metadata": {},
   "source": [
    "## 9. Conclus√£o e Discuss√£o\n",
    "\n",
    "### 9.1 Resumo dos Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc0d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_by_metric = {\n",
    "    'AUC-ROC': results_df.loc[results_df['AUC-ROC'].idxmax(), 'Model'],\n",
    "    'F1-Score': results_df.loc[results_df['F1-Score'].idxmax(), 'Model'],\n",
    "    'Recall': results_df.loc[results_df['Recall'].idxmax(), 'Model'],\n",
    "    'Precision': results_df.loc[results_df['Precision'].idxmax(), 'Model'],\n",
    "}\n",
    "\n",
    "print(\"\\nMelhor modelo por m√©trica:\")\n",
    "for metric, model in best_by_metric.items():\n",
    "    value = results_df[results_df['Model'] == model][metric].values[0]\n",
    "    print(f\"   {metric}: {model} ({value:.4f})\")\n",
    "\n",
    "print(\"TABELA RESUMO\")\n",
    "print(results_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9535c4",
   "metadata": {},
   "source": [
    "### 9.2 An√°lise dos Modelos\n",
    "\n",
    "#### **Isolation Forest**\n",
    "**Pontos Fortes:**\n",
    "- R√°pido para treinar e fazer infer√™ncia\n",
    "- N√£o assume distribui√ß√£o espec√≠fica dos dados\n",
    "- Escala bem para grandes datasets\n",
    "- Bom desempenho geral com pouca tunagem\n",
    "\n",
    "**Limita√ß√µes:**\n",
    "- Sens√≠vel ao par√¢metro de contamina√ß√£o\n",
    "- Pode n√£o capturar padr√µes locais complexos\n",
    "\n",
    "#### **Local Outlier Factor (LOF)**\n",
    "**Pontos Fortes:**\n",
    "- Considera a densidade local dos dados\n",
    "- Capaz de detectar anomalias em clusters de diferentes densidades\n",
    "- Interpret√°vel (baseado em vizinhos)\n",
    "\n",
    "**Limita√ß√µes:**\n",
    "- Computacionalmente mais caro para infer√™ncia\n",
    "- Sens√≠vel √† escolha de k (n√∫mero de vizinhos)\n",
    "- N√£o escalona t√£o bem para datasets muito grandes\n",
    "\n",
    "#### **Autoencoder**\n",
    "**Pontos Fortes:**\n",
    "- Captura rela√ß√µes n√£o-lineares complexas\n",
    "- Flex√≠vel na arquitetura\n",
    "- Aprende representa√ß√µes latentes √∫teis\n",
    "- Pode ser adaptado para diferentes tipos de dados\n",
    "\n",
    "**Limita√ß√µes:**\n",
    "- Requer mais tempo de treino\n",
    "- Necessita escolha do threshold de decis√£o\n",
    "- Pode sofrer de overfitting\n",
    "- Requer mais dados para treinar adequadamente\n",
    "\n",
    "### 9.3 Discuss√£o sobre Aplicabilidade Real\n",
    "\n",
    "**Cen√°rio de Produ√ß√£o:**\n",
    "1. **Lat√™ncia:** Para sistemas em tempo real, Isolation Forest oferece melhor tempo de infer√™ncia\n",
    "2. **Custo de Erros:** \n",
    "   - Falsos Negativos (fraudes n√£o detectadas) = preju√≠zo financeiro\n",
    "   - Falsos Positivos (transa√ß√µes leg√≠timas bloqueadas) = insatisfa√ß√£o do cliente\n",
    "3. **Trade-off Precision-Recall:** \n",
    "   - Priorizar Recall se o custo de fraude n√£o detectada for muito alto\n",
    "   - Balancear com Precision para n√£o bloquear muitas transa√ß√µes leg√≠timas\n",
    "\n",
    "**Recomenda√ß√µes:**\n",
    "1. **Para alta taxa de detec√ß√£o:** Priorizar modelo com maior Recall\n",
    "2. **Para minimizar falsos alarmes:** Priorizar modelo com maior Precision\n",
    "3. **Para equil√≠brio:** Usar F1-Score como m√©trica principal\n",
    "4. **Ensemble:** Combinar m√∫ltiplos modelos para melhor robustez\n",
    "\n",
    "### 9.4 Limita√ß√µes do Estudo\n",
    "\n",
    "1. **Dataset:**\n",
    "   - Features originais anonimizadas (PCA) limitam interpretabilidade\n",
    "   - Dados de apenas 2 dias podem n√£o capturar padr√µes sazonais\n",
    "   \n",
    "2. **Metodologia:**\n",
    "   - Abordagem semi-supervisionada assume conhecimento da propor√ß√£o de fraudes\n",
    "   - Threshold fixo pode n√£o ser √≥timo para diferentes cen√°rios\n",
    "   \n",
    "3. **Avalia√ß√£o:**\n",
    "   - M√©tricas offline podem n√£o refletir desempenho em produ√ß√£o\n",
    "   - Concept drift n√£o foi considerado\n",
    "\n",
    "### 9.5 Trabalhos Futuros\n",
    "\n",
    "1. **Ensemble de Modelos:** Combinar predi√ß√µes dos 3 modelos\n",
    "2. **Aprendizado Online:** Adaptar modelos ao longo do tempo\n",
    "3. **Features Adicionais:** Incluir padr√µes temporais e comportamentais\n",
    "4. **Interpretabilidade:** Utilizar t√©cnicas como SHAP para explicar decis√µes\n",
    "5. **Deep Learning:** Explorar arquiteturas mais complexas (LSTM, Transformer)\n",
    "6. **Federated Learning:** Treinar modelos distribu√≠dos preservando privacidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c453faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "joblib.dump(iso_forest_best, 'models/isolation_forest_best.joblib')\n",
    "\n",
    "joblib.dump(lof_best, 'models/lof_best.joblib')\n",
    "\n",
    "autoencoder.save('models/autoencoder_best.h5')\n",
    "autoencoder.save('models/autoencoder_best.h5')\n",
    "\n",
    "joblib.dump(robust_scaler, 'models/robust_scaler.joblib')\n",
    "joblib.dump(standard_scaler, 'models/standard_scaler.joblib')\n",
    "\n",
    "with open('models/autoencoder_threshold.txt', 'w') as f:\n",
    "    f.write(str(best_threshold))\n",
    "\n",
    "print(\"Modelos salvos com sucesso em ./models/\")\n",
    "print(\"   - isolation_forest_best.joblib\")\n",
    "print(\"   - lof_best.joblib\")\n",
    "print(\"   - autoencoder_best.h5\")\n",
    "print(\"   - robust_scaler.joblib\")\n",
    "print(\"   - standard_scaler.joblib\")\n",
    "print(\"   - autoencoder_threshold.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5e64d",
   "metadata": {},
   "source": [
    "## 10. Refer√™ncias\n",
    "\n",
    "1. **Dataset:**\n",
    "   - Machine Learning Group - ULB. (2013). Credit Card Fraud Detection. Kaggle.\n",
    "   - https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "\n",
    "2. **Isolation Forest:**\n",
    "   - Liu, F. T., Ting, K. M., & Zhou, Z. H. (2008). Isolation forest. In 2008 eighth ieee international conference on data mining (pp. 413-422). IEEE.\n",
    "\n",
    "3. **Local Outlier Factor:**\n",
    "   - Breunig, M. M., Kriegel, H. P., Ng, R. T., & Sander, J. (2000). LOF: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data (pp. 93-104).\n",
    "\n",
    "4. **Autoencoders for Anomaly Detection:**\n",
    "   - An, J., & Cho, S. (2015). Variational autoencoder based anomaly detection using reconstruction probability. Special Lecture on IE, 2(1), 1-18.\n",
    "\n",
    "5. **M√©tricas para Classes Desbalanceadas:**\n",
    "   - He, H., & Garcia, E. A. (2009). Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9), 1263-1284.\n",
    "\n",
    "---\n",
    "\n",
    "**Disciplina:** Aprendizado de M√°quina  \n",
    "**Institui√ß√£o:** Centro de Inform√°tica - UFPE  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
