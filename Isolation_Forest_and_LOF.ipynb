{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvh6xIjSgXa5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekA6Z5yZgZjB"
      },
      "source": [
        "## 1. Importação das Bibliotecas Necessárias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thH_CJ9rf3_C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_score, recall_score, f1_score, roc_curve,\n",
        "    precision_recall_curve, average_precision_score, accuracy_score\n",
        ")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import wilcoxon, ttest_rel\n",
        "import time\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNtnN2oBilPn",
        "outputId": "143cae4e-6602-4d46-b9c1-74a1db37142f"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 33\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJNt1_Rhncq"
      },
      "source": [
        "## 2. Carregamento dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzXSlsHEhzUR"
      },
      "outputs": [],
      "source": [
        "data_set = '/content/sample_data/creditcard.csv'\n",
        "df = pd.read_csv(data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTvjooc8h4id"
      },
      "source": [
        "2.1 Dimensão dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueqltSwDidvy",
        "outputId": "1118fef7-6762-46d7-d2db-81ba9ecdf6d3"
      },
      "outputs": [],
      "source": [
        "print(f\"Dimensões: {df.shape[0]} linhas × {df.shape[1]} colunas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1NkOuG6itOp"
      },
      "source": [
        "2.2 Tipos de dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peQARnjdiyHl",
        "outputId": "141600ef-733d-40ed-bd2a-d01b9afabb4c"
      },
      "outputs": [],
      "source": [
        "print(\"\\TIPOS DE DADOS:\")\n",
        "print(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkKDnQq2i0UF"
      },
      "source": [
        "2.3 Primeiras linhas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "F7_sktoYi8_6",
        "outputId": "81be7573-100f-4fae-fae4-63156637de9d"
      },
      "outputs": [],
      "source": [
        "print(\"\\n PRIMEIRAS 5 LINHAS:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m5nGK0Ki-5l"
      },
      "source": [
        "2.4 Estatíticas descritivas dos dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "ACLycqcmjHUA",
        "outputId": "25a9547e-b393-467c-8a01-33fe49502a80"
      },
      "outputs": [],
      "source": [
        "print(\"\\n ESTATÍSTICAS DESCRITIVAS:\")\n",
        "display(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfV2jnxxjMZy"
      },
      "source": [
        "2.5 Remoção de duplicatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbriWtkKjRRw",
        "outputId": "18f2eed6-4f1c-4f7e-82d8-1ed8f810aca4"
      },
      "outputs": [],
      "source": [
        "initial_len = df.shape[0]\n",
        "df = df.drop_duplicates()\n",
        "print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartadas {initial_len - df.shape[0]} duplicadas')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6jWxV1IjWaj"
      },
      "source": [
        "2.6 Missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoIcnO2ojb8h",
        "outputId": "967d8862-7204-4fea-893e-34ec4dadaf76"
      },
      "outputs": [],
      "source": [
        "initial_len = df.shape[0]\n",
        "df = df.dropna()\n",
        "print(f'Tamanho inicial: {initial_len}, tamanho final {df.shape[0]} | Descartados {initial_len - df.shape[0]} registros com valores NA')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCUwCPX_kCzR"
      },
      "source": [
        "2.7 Distribuição de classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srJABe21kIUG",
        "outputId": "4c7e54b7-4081-4588-bdc1-14e7eb357cb2"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nDistribuição da Classe Target (Class):\")\n",
        "print(df['Class'].value_counts())\n",
        "print(f\"\\n   Proporção de Fraudes: {df['Class'].mean()*100:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il6Jjd-Gmqyn"
      },
      "source": [
        "# 3. Análise Exploratória de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep38qcUDnLST"
      },
      "source": [
        "3.1 Análise do Desbalanceamento de Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "6lqHIibCm3pv",
        "outputId": "89bf7487-c892-4a91-faa0-9da0ef4fe590"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "class_counts = df['Class'].value_counts()\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "bars = axes[0].bar(['Normal (0)', 'Fraude (1)'], class_counts.values, color=colors, edgecolor='black')\n",
        "axes[0].set_title('Distribuição das Classes', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Número de Transações', fontsize=12)\n",
        "axes[0].set_xlabel('Classe', fontsize=12)\n",
        "\n",
        "for bar, count in zip(bars, class_counts.values):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000,\n",
        "                 f'{count:,}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "explode = (0, 0.1)\n",
        "axes[1].pie(class_counts.values, explode=explode, labels=['Normal', 'Fraude'],\n",
        "            colors=colors, autopct='%1.3f%%', shadow=True, startangle=90,\n",
        "            textprops={'fontsize': 12})\n",
        "axes[1].set_title('Proporção das Classes', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"ANÁLISE DO DESBALANCEAMENTO\")\n",
        "fraud_count = df[df['Class'] == 1].shape[0]\n",
        "normal_count = df[df['Class'] == 0].shape[0]\n",
        "print(f\"\\nTransações Normais: {normal_count:,} ({normal_count/len(df)*100:.3f}%)\")\n",
        "print(f\"Transações Fraudulentas: {fraud_count:,} ({fraud_count/len(df)*100:.3f}%)\")\n",
        "print(f\"Razão de Desbalanceamento: 1:{normal_count//fraud_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFmotPZJnQ_r"
      },
      "source": [
        " 3.2 Análise das Features Time e Amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "9nurdlSgnK_D",
        "outputId": "251ae8c2-5bee-4a13-d9e2-89a1b54bcbf8"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "fraud_df = df[df['Class'] == 1]\n",
        "normal_df = df[df['Class'] == 0]\n",
        "\n",
        "axes[0, 0].hist(normal_df['Time'], bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
        "axes[0, 0].hist(fraud_df['Time'], bins=50, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
        "axes[0, 0].set_title('Distribuição de Time por Classe', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Time (segundos)')\n",
        "axes[0, 0].set_ylabel('Densidade')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].hist(normal_df['Amount'], bins=50, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
        "axes[0, 1].hist(fraud_df['Amount'], bins=50, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
        "axes[0, 1].set_title('Distribuição de Amount por Classe', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Amount ($)')\n",
        "axes[0, 1].set_ylabel('Densidade')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].set_xlim(0, 2000)\n",
        "\n",
        "df.boxplot(column='Amount', by='Class', ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Boxplot de Amount por Classe', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Classe')\n",
        "axes[1, 0].set_ylabel('Amount ($)')\n",
        "axes[1, 0].set_ylim(0, 500)\n",
        "plt.suptitle('')\n",
        "\n",
        "df['Hour'] = (df['Time'] / 3600) % 24\n",
        "fraud_hours = df[df['Class'] == 1]['Hour']\n",
        "normal_hours = df[df['Class'] == 0]['Hour']\n",
        "\n",
        "axes[1, 1].hist(normal_hours, bins=24, alpha=0.7, label='Normal', color='#2ecc71', density=True)\n",
        "axes[1, 1].hist(fraud_hours, bins=24, alpha=0.7, label='Fraude', color='#e74c3c', density=True)\n",
        "axes[1, 1].set_title('Distribuição por Hora do Dia', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Hora do Dia')\n",
        "axes[1, 1].set_ylabel('Densidade')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('time_amount_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"ESTATÍSTICAS DE TIME E AMOUNT\")\n",
        "print(\"\\nAmount - Transações Normais:\")\n",
        "print(f\"   Média: ${normal_df['Amount'].mean():.2f}\")\n",
        "print(f\"   Mediana: ${normal_df['Amount'].median():.2f}\")\n",
        "print(f\"   Desvio Padrão: ${normal_df['Amount'].std():.2f}\")\n",
        "print(f\"   Máximo: ${normal_df['Amount'].max():.2f}\")\n",
        "\n",
        "print(\"\\nAmount - Transações Fraudulentas:\")\n",
        "print(f\"   Média: ${fraud_df['Amount'].mean():.2f}\")\n",
        "print(f\"   Mediana: ${fraud_df['Amount'].median():.2f}\")\n",
        "print(f\"   Desvio Padrão: ${fraud_df['Amount'].std():.2f}\")\n",
        "print(f\"   Máximo: ${fraud_df['Amount'].max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E_onseQnz-g"
      },
      "source": [
        "3.3 Análise das Features PCA (V1-V28)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "cb-wWwA0n2nb",
        "outputId": "840e67fe-85d0-4e3b-d587-fc108f65379b"
      },
      "outputs": [],
      "source": [
        "v_features = [f'V{i}' for i in range(1, 29)]\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(v_features[:16]):\n",
        "    axes[i].hist(normal_df[feature], bins=50, alpha=0.6, label='Normal', color='#2ecc71', density=True)\n",
        "    axes[i].hist(fraud_df[feature], bins=50, alpha=0.6, label='Fraude', color='#e74c3c', density=True)\n",
        "    axes[i].set_title(f'Distribuição de {feature}', fontsize=10)\n",
        "    axes[i].legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('v_features_distribution_1.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "6JBvkTuzoEGg",
        "outputId": "16b2e69d-4f9c-4564-893c-a47ae71891e5"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(4, 4, figsize=(20, 16))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(v_features[12:28]):\n",
        "    axes[i].hist(normal_df[feature], bins=50, alpha=0.6, label='Normal', color='#2ecc71', density=True)\n",
        "    axes[i].hist(fraud_df[feature], bins=50, alpha=0.6, label='Fraude', color='#e74c3c', density=True)\n",
        "    axes[i].set_title(f'Distribuição de {feature}', fontsize=10)\n",
        "    axes[i].legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('v_features_distribution_2.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpf0ZCYbo5Fe"
      },
      "source": [
        "3.4 Matriz de Correlação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mV_J_D3Go9iX",
        "outputId": "d8132d7d-ef64-488e-aa11-bab08b36f8c2"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(24, 20))\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='RdBu_r',\n",
        "            center=0, linewidths=0.5, fmt='.2f',\n",
        "            cbar_kws={'shrink': 0.8})\n",
        "plt.title('Matriz de Correlação das Features', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"CORRELAÇÃO DAS FEATURES COM A CLASSE TARGET\")\n",
        "correlations_with_class = df.corr()['Class'].drop('Class').sort_values(key=abs, ascending=False)\n",
        "print(\"\\nTop 10 correlações mais fortes (em valor absoluto):\")\n",
        "print(correlations_with_class.head(10))\n",
        "print(\"\\nTop 10 correlações mais fracas:\")\n",
        "print(correlations_with_class.tail(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GklWVdWzpiYQ"
      },
      "source": [
        "# 4. Pré-processamento de dados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKDmr5nGppF6"
      },
      "source": [
        "4.1 Normalização das features Time e Amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2Ow9sBGpvWs",
        "outputId": "297f43ad-ef6d-4a3d-99f4-8cc47097b126"
      },
      "outputs": [],
      "source": [
        "df_processed = df.copy()\n",
        "\n",
        "if 'Hour' in df_processed.columns:\n",
        "    df_processed = df_processed.drop('Hour', axis=1)\n",
        "\n",
        "robust_scaler = RobustScaler()\n",
        "standard_scaler = StandardScaler()\n",
        "\n",
        "df_processed['Amount_scaled'] = robust_scaler.fit_transform(df_processed['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "df_processed['Time_scaled'] = standard_scaler.fit_transform(df_processed['Time'].values.reshape(-1, 1))\n",
        "\n",
        "df_processed = df_processed.drop(['Time', 'Amount'], axis=1)\n",
        "\n",
        "print(\"Normalização concluída!\")\n",
        "print(f\"\\nShape do dataset processado: {df_processed.shape}\")\n",
        "print(f\"\\nColunas após pré-processamento:\")\n",
        "print(df_processed.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcVFm3mtqCy8"
      },
      "source": [
        "4.2 Separação dos Dados em Treino, Validação e Teste\n",
        "\n",
        "Utilizaremos a seguinte divisão:\n",
        "*   Treino: 70% dos dados\n",
        "*   Validação: 15% dos dados\n",
        "*   Teste: 15% dos dados\n",
        "\n",
        "Para detecção de anomalias em abordagem não supervisionada, treinaremos os modelos apenas com transações normais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-Wps0GzqgHr",
        "outputId": "6d6c3efa-f2c5-4ec8-8200-3a1391f7028a"
      },
      "outputs": [],
      "source": [
        "X = df_processed.drop('Class', axis=1)\n",
        "y = df_processed['Class']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.30, random_state=RANDOM_SEED, stratify=y\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"DIVISÃO DOS DADOS\")\n",
        "\n",
        "print(f\"\\nConjunto de TREINO:\")\n",
        "print(f\"   Total: {len(X_train):,} amostras ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Normal: {(y_train == 0).sum():,}\")\n",
        "print(f\"   Fraude: {(y_train == 1).sum():,}\")\n",
        "\n",
        "print(f\"\\nConjunto de VALIDAÇÃO:\")\n",
        "print(f\"   Total: {len(X_val):,} amostras ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Normal: {(y_val == 0).sum():,}\")\n",
        "print(f\"   Fraude: {(y_val == 1).sum():,}\")\n",
        "\n",
        "print(f\"\\nConjunto de TESTE:\")\n",
        "print(f\"   Total: {len(X_test):,} amostras ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "print(f\"   Normal: {(y_test == 0).sum():,}\")\n",
        "print(f\"   Fraude: {(y_test == 1).sum():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_cawX_SqoJj",
        "outputId": "28d36df3-25d8-4365-c5bf-223f54bf61b7"
      },
      "outputs": [],
      "source": [
        "X_train_normal = X_train[y_train == 0]\n",
        "y_train_normal = y_train[y_train == 0]\n",
        "\n",
        "\n",
        "print(\"CONJUNTO DE TREINO PARA ABORDAGEM NÃO SUPERVISIONADA\")\n",
        "\n",
        "print(f\"\\nTreino apenas com transações NORMAIS:\")\n",
        "print(f\"   Total: {len(X_train_normal):,} amostras\")\n",
        "print(f\"   (Usaremos este conjunto para treinar os modelos de anomalia)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk7zRbfNrfxe"
      },
      "source": [
        "# 5. Modelagem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62JZ-ESssoB6"
      },
      "source": [
        "5.1 Cálculo de métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh68grcotcd7"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, y_scores=None, model_name=\"Model\"):\n",
        "    metrics = {\n",
        "        'Model': model_name,\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'F1-Score': f1_score(y_true, y_pred, zero_division=0),\n",
        "    }\n",
        "\n",
        "    if y_scores is not None:\n",
        "        metrics['AUC-ROC'] = roc_auc_score(y_true, y_scores)\n",
        "        metrics['AUC-PR'] = average_precision_score(y_true, y_scores)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, model_name=\"Model\", ax=None):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Normal', 'Fraude'],\n",
        "                yticklabels=['Normal', 'Fraude'])\n",
        "    ax.set_xlabel('Predito')\n",
        "    ax.set_ylabel('Real')\n",
        "    ax.set_title(f'Matriz de Confusão - {model_name}')\n",
        "\n",
        "    return cm\n",
        "\n",
        "\n",
        "def print_metrics(metrics):\n",
        "    print(f\"MÉTRICAS - {metrics['Model']}\")\n",
        "    print(f\"  Precisão:  {metrics['Precision']:.4f}\")\n",
        "    print(f\"  Recall:    {metrics['Recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {metrics['F1-Score']:.4f}\")\n",
        "    if 'AUC-ROC' in metrics:\n",
        "        print(f\"  AUC-ROC:   {metrics['AUC-ROC']:.4f}\")\n",
        "    if 'AUC-PR' in metrics:\n",
        "        print(f\"  AUC-PR:    {metrics['AUC-PR']:.4f}\")\n",
        "\n",
        "all_results = {}\n",
        "all_predictions = {}\n",
        "all_scores = {}\n",
        "all_times = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ozJwTcCsVbc"
      },
      "source": [
        "5.2 Isolation Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "aP4aDV_8sYHb",
        "outputId": "8222885b-01fd-4973-dc67-e9f131cabeb4"
      },
      "outputs": [],
      "source": [
        "contamination_rate = y_val.mean()\n",
        "print(f\"Taxa de contaminação esperada: {contamination_rate:.4f} ({contamination_rate*100:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TREINAMENTO - ISOLATION FOREST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if_params = {\n",
        "    'n_estimators': 100,\n",
        "    'contamination': contamination_rate,\n",
        "    'max_samples': 'auto',\n",
        "    'random_state': 42,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "iso_forest = IsolationForest(**if_params)\n",
        "iso_forest.fit(X_train_normal)\n",
        "train_time_if = time.time() - start_time\n",
        "\n",
        "print(f\"Modelo treinado em {train_time_if:.2f} segundos\")\n",
        "\n",
        "start_time = time.time()\n",
        "if_val_pred_raw = iso_forest.predict(X_val)\n",
        "if_val_pred = np.where(if_val_pred_raw == -1, 1, 0)\n",
        "if_val_scores = -iso_forest.score_samples(X_val)\n",
        "inference_time_if_val = time.time() - start_time\n",
        "\n",
        "print(f\"Tempo de inferência (validação): {inference_time_if_val:.2f} segundos\")\n",
        "\n",
        "if_val_metrics = evaluate_model(y_val, if_val_pred, if_val_scores, \"Isolation Forest\")\n",
        "print_metrics(if_val_metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_confusion_matrix(y_val, if_val_pred, \"Isolation Forest (Validação)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_if_val.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF1Dj7ydumic"
      },
      "source": [
        "5.2.1 Tunagem de hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_e3HgyKLullY",
        "outputId": "0a9420a8-1481-49f9-a21d-004eddbdd522"
      },
      "outputs": [],
      "source": [
        "print(\"TUNAGEM DE HIPERPARÂMETROS - ISOLATION FOREST\")\n",
        "\n",
        "n_estimators_list = [50, 100, 200]\n",
        "max_samples_list = [256, 512, 'auto']\n",
        "contamination_list = [0.001, 0.002, 0.005, contamination_rate]\n",
        "\n",
        "best_if_score = 0\n",
        "best_if_params = {}\n",
        "if_tuning_results = []\n",
        "\n",
        "for n_est in n_estimators_list:\n",
        "    for max_samp in max_samples_list:\n",
        "        for contam in contamination_list:\n",
        "            try:\n",
        "                model = IsolationForest(\n",
        "                    n_estimators=n_est,\n",
        "                    max_samples=max_samp,\n",
        "                    contamination=contam,\n",
        "                    random_state=42,\n",
        "                    n_jobs=-1\n",
        "                )\n",
        "                model.fit(X_train_normal)\n",
        "\n",
        "                pred_raw = model.predict(X_val)\n",
        "                pred = np.where(pred_raw == -1, 1, 0)\n",
        "                scores = -model.score_samples(X_val)\n",
        "\n",
        "                f1 = f1_score(y_val, pred, zero_division=0)\n",
        "                auc = roc_auc_score(y_val, scores)\n",
        "\n",
        "                if_tuning_results.append({\n",
        "                    'n_estimators': n_est,\n",
        "                    'max_samples': max_samp,\n",
        "                    'contamination': contam,\n",
        "                    'F1-Score': f1,\n",
        "                    'AUC-ROC': auc\n",
        "                })\n",
        "\n",
        "                if f1 > best_if_score:\n",
        "                    best_if_score = f1\n",
        "                    best_if_params = {\n",
        "                        'n_estimators': n_est,\n",
        "                        'max_samples': max_samp,\n",
        "                        'contamination': contam\n",
        "                    }\n",
        "            except Exception as e:\n",
        "                print(f\"Erro com params {n_est}, {max_samp}, {contam}: {e}\")\n",
        "\n",
        "if_tuning_df = pd.DataFrame(if_tuning_results).sort_values('F1-Score', ascending=False)\n",
        "print(\"\\nTop 10 combinações de hiperparâmetros:\")\n",
        "print(if_tuning_df.head(10).to_string(index=False))\n",
        "\n",
        "print(f\"\\nMelhores hiperparâmetros encontrados:\")\n",
        "print(f\"   n_estimators: {best_if_params['n_estimators']}\")\n",
        "print(f\"   max_samples: {best_if_params['max_samples']}\")\n",
        "print(f\"   contamination: {best_if_params['contamination']}\")\n",
        "print(f\"   F1-Score: {best_if_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e44eS7YvUtm"
      },
      "source": [
        "5.2.3 Modelo final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlLx2X5ZvZZs",
        "outputId": "7290a687-06ce-4a37-bc42-60457a8efbfb"
      },
      "outputs": [],
      "source": [
        "print(\"MODELO FINAL - ISOLATION FOREST\")\n",
        "\n",
        "iso_forest_best = IsolationForest(\n",
        "    n_estimators=best_if_params['n_estimators'],\n",
        "    max_samples=best_if_params['max_samples'],\n",
        "    contamination=best_if_params['contamination'],\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "iso_forest_best.fit(X_train_normal)\n",
        "train_time_if_best = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "if_test_pred_raw = iso_forest_best.predict(X_test)\n",
        "if_test_pred = np.where(if_test_pred_raw == -1, 1, 0)\n",
        "if_test_scores = -iso_forest_best.score_samples(X_test)\n",
        "inference_time_if = time.time() - start_time\n",
        "\n",
        "if_test_metrics = evaluate_model(y_test, if_test_pred, if_test_scores, \"Isolation Forest (Best)\")\n",
        "print_metrics(if_test_metrics)\n",
        "print(f\"\\nTempo de treino: {train_time_if_best:.2f}s\")\n",
        "print(f\"Tempo de inferência (teste): {inference_time_if:.4f}s\")\n",
        "print(f\"Tempo médio por amostra: {inference_time_if/len(X_test)*1000:.4f}ms\")\n",
        "\n",
        "all_results['Isolation Forest'] = if_test_metrics\n",
        "all_predictions['Isolation Forest'] = if_test_pred\n",
        "all_scores['Isolation Forest'] = if_test_scores\n",
        "all_times['Isolation Forest'] = {\n",
        "    'train': train_time_if_best,\n",
        "    'inference': inference_time_if,\n",
        "    'per_sample': inference_time_if/len(X_test)*1000\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSDPlSl9wCXu"
      },
      "source": [
        "5.3 Local Outlier Factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcTr3uytw8my"
      },
      "outputs": [],
      "source": [
        "lof_params = {\n",
        "    'n_neighbors': 20,\n",
        "    'contamination': contamination_rate,\n",
        "    'novelty': True,\n",
        "    'n_jobs': -1\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "lof = LocalOutlierFactor(**lof_params)\n",
        "lof.fit(X_train_normal)\n",
        "train_time_lof = time.time() - start_time\n",
        "\n",
        "print(f\"Modelo treinado em {train_time_lof:.2f} segundos\")\n",
        "\n",
        "start_time = time.time()\n",
        "lof_val_pred_raw = lof.predict(X_val)\n",
        "lof_val_pred = np.where(lof_val_pred_raw == -1, 1, 0)\n",
        "lof_val_scores = -lof.score_samples(X_val)\n",
        "inference_time_lof_val = time.time() - start_time\n",
        "\n",
        "print(f\"Tempo de inferência (validação): {inference_time_lof_val:.2f} segundos\")\n",
        "\n",
        "lof_val_metrics = evaluate_model(y_val, lof_val_pred, lof_val_scores, \"LOF\")\n",
        "print_metrics(lof_val_metrics)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plot_confusion_matrix(y_val, lof_val_pred, \"LOF (Validação)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('confusion_matrix_lof_val.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4WpjOhfxjN5"
      },
      "source": [
        "5.3.1 Tunagem de hiperparâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqtQacrMxprk"
      },
      "outputs": [],
      "source": [
        "n_neighbors_list = [5, 10, 20, 30, 50]\n",
        "contamination_list_lof = [0.001, 0.002, 0.005, contamination_rate]\n",
        "\n",
        "best_lof_score = 0\n",
        "best_lof_params = {}\n",
        "lof_tuning_results = []\n",
        "\n",
        "for n_neigh in n_neighbors_list:\n",
        "    for contam in contamination_list_lof:\n",
        "        try:\n",
        "            model = LocalOutlierFactor(\n",
        "                n_neighbors=n_neigh,\n",
        "                contamination=contam,\n",
        "                novelty=True,\n",
        "                n_jobs=-1\n",
        "            )\n",
        "            model.fit(X_train_normal)\n",
        "\n",
        "            pred_raw = model.predict(X_val)\n",
        "            pred = np.where(pred_raw == -1, 1, 0)\n",
        "            scores = -model.score_samples(X_val)\n",
        "\n",
        "            f1 = f1_score(y_val, pred, zero_division=0)\n",
        "            auc = roc_auc_score(y_val, scores)\n",
        "\n",
        "            lof_tuning_results.append({\n",
        "                'n_neighbors': n_neigh,\n",
        "                'contamination': contam,\n",
        "                'F1-Score': f1,\n",
        "                'AUC-ROC': auc\n",
        "            })\n",
        "\n",
        "            if f1 > best_lof_score:\n",
        "                best_lof_score = f1\n",
        "                best_lof_params = {\n",
        "                    'n_neighbors': n_neigh,\n",
        "                    'contamination': contam\n",
        "                }\n",
        "        except Exception as e:\n",
        "            print(f\"Erro com params {n_neigh}, {contam}: {e}\")\n",
        "\n",
        "lof_tuning_df = pd.DataFrame(lof_tuning_results).sort_values('F1-Score', ascending=False)\n",
        "print(\"\\nTop 10 combinações de hiperparâmetros:\")\n",
        "print(lof_tuning_df.head(10).to_string(index=False))\n",
        "\n",
        "print(f\"\\nMelhores hiperparâmetros encontrados:\")\n",
        "print(f\"   n_neighbors: {best_lof_params['n_neighbors']}\")\n",
        "print(f\"   contamination: {best_lof_params['contamination']}\")\n",
        "print(f\"   F1-Score: {best_lof_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipYu-nPXxx0Q"
      },
      "outputs": [],
      "source": [
        "print(\"MODELO FINAL - LOF\")\n",
        "\n",
        "lof_best = LocalOutlierFactor(\n",
        "    n_neighbors=best_lof_params['n_neighbors'],\n",
        "    contamination=best_lof_params['contamination'],\n",
        "    novelty=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "lof_best.fit(X_train_normal)\n",
        "train_time_lof_best = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "lof_test_pred_raw = lof_best.predict(X_test)\n",
        "lof_test_pred = np.where(lof_test_pred_raw == -1, 1, 0)\n",
        "lof_test_scores = -lof_best.score_samples(X_test)\n",
        "inference_time_lof = time.time() - start_time\n",
        "\n",
        "lof_test_metrics = evaluate_model(y_test, lof_test_pred, lof_test_scores, \"LOF (Best)\")\n",
        "print_metrics(lof_test_metrics)\n",
        "print(f\"\\nTempo de treino: {train_time_lof_best:.2f}s\")\n",
        "print(f\"Tempo de inferência (teste): {inference_time_lof:.4f}s\")\n",
        "print(f\"Tempo médio por amostra: {inference_time_lof/len(X_test)*1000:.4f}ms\")\n",
        "\n",
        "all_results['LOF'] = lof_test_metrics\n",
        "all_predictions['LOF'] = lof_test_pred\n",
        "all_scores['LOF'] = lof_test_scores\n",
        "all_times['LOF'] = {\n",
        "    'train': train_time_lof_best,\n",
        "    'inference': inference_time_lof,\n",
        "    'per_sample': inference_time_lof/len(X_test)*1000\n",
        "}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
